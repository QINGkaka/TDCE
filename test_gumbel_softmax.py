"""
TDCE: æµ‹è¯•Gumbel-Softmaxæ‰©æ•£åŠŸèƒ½
"""

import torch
import numpy as np
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from tdce.gumbel_softmax_utils import (
    gumbel_softmax_relaxation,
    temperature_scheduler,
    gumbel_softmax_q_sample,
    index_to_onehot,
    gumbel_softmax_to_index
)

def test_gumbel_softmax_utils():
    """æµ‹è¯•Gumbel-Softmaxå·¥å…·å‡½æ•°"""
    print("=" * 60)
    print("æµ‹è¯•1: Gumbel-Softmaxå·¥å…·å‡½æ•°")
    print("=" * 60)
    
    # æµ‹è¯•1.1: gumbel_softmax_relaxation
    print("\n1.1 æµ‹è¯• gumbel_softmax_relaxation...")
    batch_size = 4
    num_classes = 5
    logits = torch.randn(batch_size, num_classes)
    tau = 1.0
    result = gumbel_softmax_relaxation(logits, tau=tau, hard=False)
    print(f"   è¾“å…¥logits shape: {logits.shape}")
    print(f"   è¾“å‡ºshape: {result.shape}")
    print(f"   è¾“å‡ºå’Œæ˜¯å¦ä¸º1: {result.sum(dim=-1)}")  # åº”è¯¥æ¥è¿‘1
    assert result.shape == (batch_size, num_classes), f"Shapeé”™è¯¯: {result.shape}"
    assert torch.allclose(result.sum(dim=-1), torch.ones(batch_size), atol=1e-3), "æ¦‚ç‡å’Œä¸ç­‰äº1"
    print("   âœ… é€šè¿‡")
    
    # æµ‹è¯•1.2: temperature_scheduler
    print("\n1.2 æµ‹è¯• temperature_scheduler...")
    tau_init = 1.0
    tau_final = 0.3
    num_timesteps = 1000
    tau_start = temperature_scheduler(0, tau_init, tau_final, num_timesteps)
    tau_end = temperature_scheduler(num_timesteps-1, tau_init, tau_final, num_timesteps)
    print(f"   åˆå§‹æ¸©åº¦: {tau_start:.4f} (æœŸæœ›: {tau_init})")
    print(f"   æœ€ç»ˆæ¸©åº¦: {tau_end:.4f} (æœŸæœ›: {tau_final})")
    assert abs(tau_start - tau_init) < 1e-6, f"åˆå§‹æ¸©åº¦é”™è¯¯: {tau_start}"
    assert abs(tau_end - tau_final) < 1e-3, f"æœ€ç»ˆæ¸©åº¦é”™è¯¯: {tau_end}"
    print("   âœ… é€šè¿‡")
    
    # æµ‹è¯•1.3: index_to_onehot
    print("\n1.3 æµ‹è¯• index_to_onehot...")
    batch_size = 4
    num_cat_features = 3
    num_classes = [2, 3, 4]  # æ¯ä¸ªåˆ†ç±»ç‰¹å¾çš„ç±»åˆ«æ•°
    # ä¸ºæ¯ä¸ªç‰¹å¾ç”Ÿæˆç¬¦åˆå…¶ç±»åˆ«æ•°èŒƒå›´çš„ç´¢å¼•
    x_index = torch.zeros(batch_size, num_cat_features, dtype=torch.long)
    for i in range(num_cat_features):
        x_index[:, i] = torch.randint(0, num_classes[i], (batch_size,))
    x_onehot = index_to_onehot(x_index, num_classes)
    print(f"   è¾“å…¥ç´¢å¼• shape: {x_index.shape}")
    print(f"   è¾“å‡ºone-hot shape: {x_onehot.shape}")
    print(f"   ç¤ºä¾‹è¾“å…¥: {x_index[0]}")
    print(f"   ç¤ºä¾‹è¾“å‡º shape: {x_onehot[0].shape}")
    assert x_onehot.shape == (batch_size, num_cat_features, max(num_classes)), f"Shapeé”™è¯¯: {x_onehot.shape}"
    print("   âœ… é€šè¿‡")
    
    # æµ‹è¯•1.4: gumbel_softmax_q_sample
    print("\n1.4 æµ‹è¯• gumbel_softmax_q_sample...")
    batch_size = 4
    num_cat_features = 2
    num_classes_per_feat = 3
    x_cat_onehot = torch.zeros(batch_size, num_cat_features, num_classes_per_feat)
    for i in range(batch_size):
        for j in range(num_cat_features):
            idx = np.random.randint(0, num_classes_per_feat)
            x_cat_onehot[i, j, idx] = 1.0
    
    t = torch.randint(0, 1000, (batch_size,))
    beta_schedule = torch.linspace(0.0001, 0.02, 1000)
    tau = 1.0
    device = torch.device('cpu')
    
    x_t_cat = gumbel_softmax_q_sample(x_cat_onehot, t, beta_schedule, tau, device)
    print(f"   è¾“å…¥one-hot shape: {x_cat_onehot.shape}")
    print(f"   è¾“å‡ºGumbel-Softmax shape: {x_t_cat.shape}")
    print(f"   è¾“å‡ºæ¦‚ç‡å’Œ: {x_t_cat.sum(dim=-1)[0]}")  # åº”è¯¥æ¥è¿‘1
    assert x_t_cat.shape == x_cat_onehot.shape, f"Shapeé”™è¯¯: {x_t_cat.shape}"
    assert torch.allclose(x_t_cat.sum(dim=-1), torch.ones(batch_size, num_cat_features), atol=1e-2), "æ¦‚ç‡å’Œä¸ç­‰äº1"
    print("   âœ… é€šè¿‡")
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰Gumbel-Softmaxå·¥å…·å‡½æ•°æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 60)


def test_diffusion_with_gumbel_softmax():
    """æµ‹è¯•æ‰©æ•£æ¨¡å‹ä¸­çš„Gumbel-SoftmaxåŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•2: æ‰©æ•£æ¨¡å‹Gumbel-Softmaxé›†æˆ")
    print("=" * 60)
    
    try:
        from tdce.gaussian_multinomial_diffsuion import GaussianMultinomialDiffusion
        from tdce.modules import MLPDiffusion
        
        # åˆ›å»ºç®€å•çš„æ¨¡å‹
        num_classes = np.array([2, 3])  # 2ä¸ªåˆ†ç±»ç‰¹å¾ï¼Œåˆ†åˆ«æœ‰2å’Œ3ä¸ªç±»åˆ«
        num_numerical_features = 5
        num_timesteps = 100
        
        # åˆ›å»ºç®€å•çš„å»å™ªç½‘ç»œ
        # MLPDiffusionéœ€è¦çš„å‚æ•°ï¼šd_in, num_classes, is_y_cond, rtdl_params, dim_t
        rtdl_params = {
            'd_layers': [64, 64],
            'dropout': 0.0
        }
        model = MLPDiffusion(
            d_in=num_numerical_features + sum(num_classes),  # æ•°å€¼ç‰¹å¾ + åˆ†ç±»ç‰¹å¾çš„æ€»ç»´åº¦
            num_classes=0,  # ä¸ä½¿ç”¨æ¡ä»¶æ ‡ç­¾
            is_y_cond=False,  # ä¸ä½¿ç”¨æ¡ä»¶
            rtdl_params=rtdl_params,
            dim_t=128  # æ—¶é—´åµŒå…¥ç»´åº¦
        )
        
        # æµ‹è¯•2.1: åˆ›å»ºæ‰©æ•£æ¨¡å‹ï¼ˆä½¿ç”¨Gumbel-Softmaxï¼‰
        print("\n2.1 æµ‹è¯•åˆ›å»ºæ‰©æ•£æ¨¡å‹ï¼ˆuse_gumbel_softmax=Trueï¼‰...")
        diffusion = GaussianMultinomialDiffusion(
            num_classes=num_classes,
            num_numerical_features=num_numerical_features,
            denoise_fn=model,
            num_timesteps=num_timesteps,
            use_gumbel_softmax=True,
            tau_init=1.0,
            tau_final=0.3,
            tau_schedule='anneal',
            device=torch.device('cpu')
        )
        print(f"   æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"   use_gumbel_softmax: {diffusion.use_gumbel_softmax}")
        print(f"   betas shape: {diffusion.betas.shape}")
        print("   âœ… é€šè¿‡")
        
        # æµ‹è¯•2.2: q_sample_gumbel_softmax
        print("\n2.2 æµ‹è¯• q_sample_gumbel_softmax...")
        batch_size = 8
        # ä¸ºæ¯ä¸ªç‰¹å¾ç”Ÿæˆç¬¦åˆå…¶ç±»åˆ«æ•°èŒƒå›´çš„ç´¢å¼•
        x_cat_index = torch.zeros(batch_size, len(num_classes), dtype=torch.long)
        for i in range(len(num_classes)):
            x_cat_index[:, i] = torch.randint(0, num_classes[i], (batch_size,))
        x_cat_onehot = index_to_onehot(x_cat_index, list(num_classes))
        t = torch.randint(0, num_timesteps, (batch_size,))
        
        x_t_cat = diffusion.q_sample_gumbel_softmax(x_cat_onehot, t)
        print(f"   è¾“å…¥one-hot shape: {x_cat_onehot.shape}")
        print(f"   è¾“å‡ºGumbel-Softmax shape: {x_t_cat.shape}")
        assert x_t_cat.shape == x_cat_onehot.shape, f"Shapeé”™è¯¯: {x_t_cat.shape}"
        print("   âœ… é€šè¿‡")
        
        # æµ‹è¯•2.3: mixed_lossï¼ˆä½¿ç”¨Gumbel-Softmaxï¼‰
        print("\n2.3 æµ‹è¯• mixed_lossï¼ˆuse_gumbel_softmax=Trueï¼‰...")
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        x_num = torch.randn(batch_size, num_numerical_features)
        # ä¸ºæ¯ä¸ªç‰¹å¾ç”Ÿæˆç¬¦åˆå…¶ç±»åˆ«æ•°èŒƒå›´çš„ç´¢å¼•
        x_cat = torch.zeros(batch_size, len(num_classes), dtype=torch.long)
        for i in range(len(num_classes)):
            x_cat[:, i] = torch.randint(0, num_classes[i], (batch_size,))
        x = torch.cat([x_num, x_cat.float()], dim=1)
        
        # åˆ›å»ºout_dict
        out_dict = {
            'y': torch.randint(0, 2, (batch_size,))  # å‡è®¾æ˜¯äºŒåˆ†ç±»ä»»åŠ¡
        }
        
        try:
            loss_multi, loss_gauss = diffusion.mixed_loss(x, out_dict)
            print(f"   åˆ†ç±»ç‰¹å¾æŸå¤±: {loss_multi.item():.6f}")
            print(f"   æ•°å€¼ç‰¹å¾æŸå¤±: {loss_gauss.item():.6f}")
            assert not torch.isnan(loss_multi), "åˆ†ç±»ç‰¹å¾æŸå¤±ä¸ºNaN"
            assert not torch.isnan(loss_gauss), "æ•°å€¼ç‰¹å¾æŸå¤±ä¸ºNaN"
            assert loss_multi >= 0, f"åˆ†ç±»ç‰¹å¾æŸå¤±ä¸ºè´Ÿ: {loss_multi}"
            assert loss_gauss >= 0, f"æ•°å€¼ç‰¹å¾æŸå¤±ä¸ºè´Ÿ: {loss_gauss}"
            print("   âœ… é€šè¿‡")
        except Exception as e:
            print(f"   âŒ å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        # æµ‹è¯•2.4: å¯¹æ¯”ä¼ ç»Ÿå¤šé¡¹å¼æ‰©æ•£ï¼ˆuse_gumbel_softmax=Falseï¼‰
        print("\n2.4 æµ‹è¯• mixed_lossï¼ˆuse_gumbel_softmax=Falseï¼Œå¯¹æ¯”ï¼‰...")
        diffusion_traditional = GaussianMultinomialDiffusion(
            num_classes=num_classes,
            num_numerical_features=num_numerical_features,
            denoise_fn=model,
            num_timesteps=num_timesteps,
            use_gumbel_softmax=False,
            device=torch.device('cpu')
        )
        
        try:
            loss_multi_trad, loss_gauss_trad = diffusion_traditional.mixed_loss(x, out_dict)
            print(f"   ä¼ ç»Ÿæ–¹æ³•åˆ†ç±»æŸå¤±: {loss_multi_trad.item():.6f}")
            print(f"   ä¼ ç»Ÿæ–¹æ³•æ•°å€¼æŸå¤±: {loss_gauss_trad.item():.6f}")
            print("   âœ… ä¼ ç»Ÿæ–¹æ³•ä¹Ÿæ­£å¸¸å·¥ä½œ")
        except Exception as e:
            print(f"   âš ï¸  ä¼ ç»Ÿæ–¹æ³•æµ‹è¯•å¤±è´¥ï¼ˆå¯èƒ½ä¸å½±å“TDCEï¼‰: {e}")
        
        print("\n" + "=" * 60)
        print("âœ… æ‰©æ•£æ¨¡å‹Gumbel-Softmaxé›†æˆæµ‹è¯•é€šè¿‡ï¼")
        print("=" * 60)
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_p_sample_gumbel_softmax():
    """æµ‹è¯•p_sample_gumbel_softmaxåå‘é‡‡æ ·æ–¹æ³•"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•3: p_sample_gumbel_softmaxåå‘é‡‡æ ·")
    print("=" * 60)
    
    try:
        from tdce.gaussian_multinomial_diffsuion import GaussianMultinomialDiffusion
        from tdce.modules import MLPDiffusion
        
        # åˆ›å»ºç®€å•çš„æ¨¡å‹
        num_classes = np.array([2, 3])  # 2ä¸ªåˆ†ç±»ç‰¹å¾ï¼Œåˆ†åˆ«æœ‰2å’Œ3ä¸ªç±»åˆ«
        num_numerical_features = 5
        num_timesteps = 100
        
        # åˆ›å»ºç®€å•çš„å»å™ªç½‘ç»œ
        rtdl_params = {
            'd_layers': [64, 64],
            'dropout': 0.0
        }
        model = MLPDiffusion(
            d_in=num_numerical_features + sum(num_classes),
            num_classes=0,
            is_y_cond=False,
            rtdl_params=rtdl_params,
            dim_t=128
        )
        
        # åˆ›å»ºæ‰©æ•£æ¨¡å‹ï¼ˆä½¿ç”¨Gumbel-Softmaxï¼‰
        print("\n3.1 æµ‹è¯•åˆ›å»ºæ‰©æ•£æ¨¡å‹...")
        diffusion = GaussianMultinomialDiffusion(
            num_classes=num_classes,
            num_numerical_features=num_numerical_features,
            denoise_fn=model,
            num_timesteps=num_timesteps,
            use_gumbel_softmax=True,
            tau_init=1.0,
            tau_final=0.3,
            tau_schedule='anneal',
            device=torch.device('cpu')
        )
        print("   âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•3.2: p_sample_gumbel_softmax
        print("\n3.2 æµ‹è¯• p_sample_gumbel_softmax...")
        batch_size = 4
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„æ¨¡å‹è¾“å‡ºå’Œå½“å‰æ—¶é—´æ­¥çš„åˆ†ç±»ç‰¹å¾
        from tdce.gumbel_softmax_utils import index_to_onehot
        
        # åˆ›å»ºéšæœºçš„åˆ†ç±»ç‰¹å¾ç´¢å¼•
        x_cat_index = torch.zeros(batch_size, len(num_classes), dtype=torch.long)
        for i in range(len(num_classes)):
            x_cat_index[:, i] = torch.randint(0, num_classes[i], (batch_size,))
        
        # è½¬ä¸ºone-hotï¼Œç„¶åé€šè¿‡å‰å‘æ‰©æ•£å¾—åˆ°x_t
        x_cat_onehot = index_to_onehot(x_cat_index, list(num_classes))
        t = torch.randint(50, 100, (batch_size,))  # éšæœºæ—¶é—´æ­¥
        
        # å‰å‘æ‰©æ•£å¾—åˆ°x_t
        x_t_cat_gumbel = diffusion.q_sample_gumbel_softmax(x_cat_onehot, t)
        print(f"   è¾“å…¥x_t_cat_gumbel shape: {x_t_cat_gumbel.shape}")
        
        # æ¨¡æ‹Ÿæ¨¡å‹è¾“å‡ºï¼ˆéšæœºlogitsï¼‰
        model_out_cat = torch.randn(batch_size, sum(num_classes))
        
        # åå‘é‡‡æ ·
        out_dict = {'y': torch.randint(0, 2, (batch_size,))}
        x_t_minus_1_cat = diffusion.p_sample_gumbel_softmax(
            model_out_cat,
            x_t_cat_gumbel,
            t,
            out_dict
        )
        print(f"   è¾“å‡ºx_t_minus_1_cat shape: {x_t_minus_1_cat.shape}")
        print(f"   è¾“å‡ºæ¦‚ç‡å’Œï¼ˆæ¯ä¸ªç‰¹å¾ï¼‰: {x_t_minus_1_cat.sum(dim=-1)[0]}")
        
        # éªŒè¯å½¢çŠ¶å’Œæ¦‚ç‡å’Œ
        assert x_t_minus_1_cat.shape == x_t_cat_gumbel.shape, f"å½¢çŠ¶é”™è¯¯: {x_t_minus_1_cat.shape} vs {x_t_cat_gumbel.shape}"
        # éªŒè¯æ¯ä¸ªåˆ†ç±»ç‰¹å¾çš„æ¦‚ç‡å’Œæ¥è¿‘1
        for i in range(len(num_classes)):
            prob_sum = x_t_minus_1_cat[:, i, :num_classes[i]].sum(dim=-1)
            assert torch.allclose(prob_sum, torch.ones(batch_size), atol=1e-3), f"ç‰¹å¾{i}çš„æ¦‚ç‡å’Œä¸ç­‰äº1"
        
        print("   âœ… é€šè¿‡")
        
        # æµ‹è¯•3.3: å®Œæ•´çš„sampleæ–¹æ³•ï¼ˆGumbel-Softmaxæ¨¡å¼ï¼‰
        print("\n3.3 æµ‹è¯•å®Œæ•´çš„sampleæ–¹æ³•ï¼ˆGumbel-Softmaxæ¨¡å¼ï¼‰...")
        y_dist = torch.tensor([0.5, 0.5])  # äºŒåˆ†ç±»ï¼Œå‡åŒ€åˆ†å¸ƒ
        
        sample, out_dict = diffusion.sample(num_samples=4, y_dist=y_dist)
        print(f"   é‡‡æ ·ç»“æœshape: {sample.shape}")
        print(f"   æœŸæœ›shape: (4, {num_numerical_features + len(num_classes)})")
        
        # éªŒè¯å½¢çŠ¶
        expected_shape = (4, num_numerical_features + len(num_classes))
        assert sample.shape == expected_shape, f"å½¢çŠ¶é”™è¯¯: {sample.shape} vs {expected_shape}"
        
        # éªŒè¯åˆ†ç±»ç‰¹å¾å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
        if len(num_classes) > 0:
            x_cat_sampled = sample[:, num_numerical_features:]
            for i, num_class in enumerate(num_classes):
                assert (x_cat_sampled[:, i] >= 0).all(), f"ç‰¹å¾{i}æœ‰è´Ÿå€¼"
                assert (x_cat_sampled[:, i] < num_class).all(), f"ç‰¹å¾{i}è¶…å‡ºèŒƒå›´"
        
        print("   âœ… é€šè¿‡")
        
        # æµ‹è¯•3.4: å¯¹æ¯”ä¼ ç»Ÿæ¨¡å¼ï¼ˆuse_gumbel_softmax=Falseï¼‰
        print("\n3.4 æµ‹è¯•ä¼ ç»Ÿæ¨¡å¼ï¼ˆuse_gumbel_softmax=Falseï¼Œå¯¹æ¯”ï¼‰...")
        diffusion_traditional = GaussianMultinomialDiffusion(
            num_classes=num_classes,
            num_numerical_features=num_numerical_features,
            denoise_fn=model,
            num_timesteps=num_timesteps,
            use_gumbel_softmax=False,
            device=torch.device('cpu')
        )
        
        sample_trad, out_dict_trad = diffusion_traditional.sample(num_samples=4, y_dist=y_dist)
        print(f"   ä¼ ç»Ÿæ–¹æ³•é‡‡æ ·ç»“æœshape: {sample_trad.shape}")
        assert sample_trad.shape == expected_shape, f"ä¼ ç»Ÿæ–¹æ³•å½¢çŠ¶é”™è¯¯: {sample_trad.shape}"
        print("   âœ… ä¼ ç»Ÿæ–¹æ³•ä¹Ÿæ­£å¸¸å·¥ä½œ")
        
        print("\n" + "=" * 60)
        print("âœ… p_sample_gumbel_softmaxå’Œsampleæ–¹æ³•æµ‹è¯•é€šè¿‡ï¼")
        print("=" * 60)
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 60)
    print("TDCE Gumbel-SoftmaxåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•1: Gumbel-Softmaxå·¥å…·å‡½æ•°
    test_gumbel_softmax_utils()
    
    # æµ‹è¯•2: æ‰©æ•£æ¨¡å‹é›†æˆ
    success2 = test_diffusion_with_gumbel_softmax()
    
    # æµ‹è¯•3: p_sample_gumbel_softmaxåå‘é‡‡æ ·
    success3 = test_p_sample_gumbel_softmax()
    
    print("\n" + "=" * 60)
    if success2 and success3:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Gumbel-SoftmaxåŠŸèƒ½æ­£å¸¸")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    print("=" * 60)


if __name__ == '__main__':
    main()

